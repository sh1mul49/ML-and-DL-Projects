{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string #punctuations, data preprocessing\n",
    "import numpy as np\n",
    "import io\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #data encoding\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer #5th step - data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize():\n",
    "    file = open('corpus.txt','r',errors='ignore')\n",
    "    corpus = file.read()\n",
    "    sentence_tokens = nltk.sent_tokenize(corpus)\n",
    "    word_tokens = nltk.word_tokenize(corpus)\n",
    "    return sentence_tokens, word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemtokens(tokens):\n",
    "    lst=[] \n",
    "    for i in tokens: #every individual token have to be lemmatized\n",
    "        lst.append(lemmatizer.lemmatize(i))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove the punctuation\n",
    "punct = dict((ord(i),None) for i in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmer(text):\n",
    "    #Tokenize\n",
    "    tokenized_text=nltk.word_tokenize(text.lower().translate(punct))\n",
    "    lemmatized_values = lemtokens(tokenized_text)\n",
    "    return lemmatized_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Greeting - Rule Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_inputs = ['hello','hi','hey','greetings']\n",
    "greeting_responses = ['I am a chatbot','hello','hi','whats up','hi there']\n",
    "\n",
    "def greeting(text):\n",
    "    #Tokenize\n",
    "    for token in text.split():\n",
    "        #inputs\n",
    "        if token.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf = [t1,t2,t3,t4,user_query]\n",
    "\n",
    "sim = [s1,s2,s3,s4,s5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate response for the queries from the corpus!\n",
    "\n",
    "Data Encoding - TFIDF\n",
    "Similarity Metric - cosine similarity\n",
    "choosing the vector with maximum similarity in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(user_query):\n",
    "    bot_response=''\n",
    "    \n",
    "    #Tokenize\n",
    "    sent_tokens,word_tokens = tokenize()\n",
    "    sent_tokens.append(user_query)\n",
    "    \n",
    "    #Vectorizing\n",
    "    tfidf_obj = TfidfVectorizer(tokenizer=lemmer,stop_words=\"english\")\n",
    "    tfidf = tfidf_obj.fit_transform(sent_tokens)\n",
    "    \n",
    "    #cosine similarity\n",
    "    sim_values = cosine_similarity(tfidf[-1],tfidf) #cosine similarity of the last element with the entire list\n",
    "    \n",
    "    #selecting the response or token with max similarity\n",
    "    index = sim_values.argsort()[0][-2]\n",
    "    \n",
    "    flattened_sim = sim_values.flatten()\n",
    "    flattened_sim.sort()\n",
    "    \n",
    "    required_tfidf = flattened_sim[-2]\n",
    "    \n",
    "    if(required_tfidf==0):\n",
    "        bot_response += 'I cannot understand'\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response += bot_response + sent_tokens[index]\n",
    "        return bot_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot\n",
      "Hi\n",
      "Bot: whats up\n",
      "hello\n",
      "Bot: hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot\")\n",
    "flag = 1\n",
    "\n",
    "while(flag == 1):\n",
    "    user_query = input()\n",
    "    user_query = user_query.lower()\n",
    "    \n",
    "    #exit\n",
    "    if(user_query=='exit'):\n",
    "        flag=0\n",
    "        print('Bot: Bye C u!')\n",
    "    \n",
    "    else:\n",
    "        #greeting\n",
    "        if(greeting(user_query) != None):\n",
    "            print(\"Bot: \"+greeting(user_query))\n",
    "        \n",
    "        #general\n",
    "        else:\n",
    "            res = respond(user_query)\n",
    "            print(\"Bot: \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
